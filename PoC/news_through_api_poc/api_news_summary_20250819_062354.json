{
  "timestamp": "2025-08-19T06:23:54.864809",
  "total_items": 30,
  "sources_summary": {
    "arxiv": {
      "count": 30,
      "sources": [
        "ArXiv Computer Vision",
        "ArXiv AI Papers",
        "ArXiv NLP Papers"
      ]
    },
    "newsapi": {
      "count": 0,
      "sources": []
    },
    "paperswithcode": {
      "count": 0,
      "sources": []
    }
  },
  "categories_summary": {
    "Research Papers": 20,
    "Computer Vision": 8,
    "Natural Language Processing": 2
  },
  "items": [
    {
      "title": "Thyme: Think Beyond Images",
      "link": "http://arxiv.org/abs/2508.11630v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
      "published_date": "2025-08-15",
      "content_hash": "0df72ae8f4158916e233034b27c3756d",
      "api_data": {
        "authors": [
          "Yi-Fan Zhang",
          "Xingyu Lu",
          "Shukang Yin",
          "Chaoyou Fu",
          "Wei Chen",
          "Xiao Hu",
          "Bin Wen",
          "Kaiyu Jiang",
          "Changyi Liu",
          "Tianke Zhang",
          "Haonan Fan",
          "Kaibing Chen",
          "Jiankang Chen",
          "Haojie Ding",
          "Kaiyu Tang",
          "Zhang Zhang",
          "Liang Wang",
          "Fan Yang",
          "Tingting Gao",
          "Guorui Zhou"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11630v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Is ChatGPT-5 Ready for Mammogram VQA?",
      "link": "http://arxiv.org/abs/2508.11628v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.",
      "published_date": "2025-08-15",
      "content_hash": "931e9e3f6aa5979b9be2fe64733662b7",
      "api_data": {
        "authors": [
          "Qiang Li",
          "Shansong Wang",
          "Mingzhe Hu",
          "Mojtaba Safari",
          "Zachary Eidex",
          "Xiaofeng Yang"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11628v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Deconfounding via Profiled Transfer Learning",
      "link": "http://arxiv.org/abs/2508.11622v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Unmeasured confounders are a major source of bias in regression-based effect\nestimation and causal inference. In this paper, we advocate a new profiled\ntransfer learning framework, ProTrans, to address confounding effects in the\ntarget dataset, when additional source datasets that possess similar\nconfounding structures are available. We introduce the concept of profiled\nresiduals to characterize the shared confounding patterns between source and\ntarget datasets. By incorporating these profiled residuals into the target\ndebiasing step, we effectively mitigates the latent confounding effects. We\nalso propose a source selection strategy to enhance robustness of ProTrans\nagainst noninformative sources. As a byproduct, ProTrans can also be utilized\nto estimate treatment effects when potential confounders exist, without the use\nof auxiliary features such as instrumental or proxy variables, which are often\nchallenging to select in practice. Theoretically, we prove that the resulting\nestimated model shift from sources to target is confounding-free without any\nassumptions imposed on the true confounding structure, and that the target\nparameter estimation achieves the minimax optimal rate under mild conditions.\nSimulated and real-world experiments validate the effectiveness of ProTrans and\nsupport the theoretical findings.",
      "published_date": "2025-08-15",
      "content_hash": "f9290946a4b1f511ca6e4b981394ce49",
      "api_data": {
        "authors": [
          "Ziyuan Chen",
          "Yifan Jiang",
          "Jingyuan Liu",
          "Fang Yao"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11622v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Grab-n-Go: On-the-Go Microgesture Recognition with Objects in Hand",
      "link": "http://arxiv.org/abs/2508.11620v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "As computing devices become increasingly integrated into daily life, there is\na growing need for intuitive, always-available interaction methods, even when\nusers' hands are occupied. In this paper, we introduce Grab-n-Go, the first\nwearable device that leverages active acoustic sensing to recognize subtle hand\nmicrogestures while holding various objects. Unlike prior systems that focus\nsolely on free-hand gestures or basic hand-object activity recognition,\nGrab-n-Go simultaneously captures information about hand microgestures,\ngrasping poses, and object geometries using a single wristband, enabling the\nrecognition of fine-grained hand movements occurring within activities\ninvolving occupied hands. A deep learning framework processes these complex\nsignals to identify 30 distinct microgestures, with 6 microgestures for each of\nthe 5 grasping poses. In a user study with 10 participants and 25 everyday\nobjects, Grab-n-Go achieved an average recognition accuracy of 92.0%. A\nfollow-up study further validated Grab-n-Go's robustness against 10 more\nchallenging, deformable objects. These results underscore the potential of\nGrab-n-Go to provide seamless, unobtrusive interactions without requiring\nmodifications to existing objects. The complete dataset, comprising data from\n18 participants performing 30 microgestures with 35 distinct objects, is\npublicly available at https://github.com/cjlisalee/Grab-n-Go_Data with the DOI:\nhttps://doi.org/10.7298/7kbd-vv75.",
      "published_date": "2025-08-15",
      "content_hash": "c2e125e4663cad109d94f373b041cd2c",
      "api_data": {
        "authors": [
          "Chi-Jung Lee",
          "Jiaxin Li",
          "Tianhong Catherine Yu",
          "Ruidong Zhang",
          "Vipin Gunda",
          "François Guimbretière",
          "Cheng Zhang"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11620v1",
        "journal_ref": null,
        "doi": "10.1145/3749469"
      }
    },
    {
      "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective",
      "link": "http://arxiv.org/abs/2508.11618v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Carbon capture and storage (CCS) projects typically involve a diverse array\nof stakeholders or players from public, private, and regulatory sectors, each\nwith different objectives and responsibilities. Given the complexity, scale,\nand long-term nature of CCS operations, determining whether individual\nstakeholders can independently maximize their interests or whether\ncollaborative coalition agreements are needed remains a central question for\neffective CCS project planning and management. CCS projects are often\nimplemented in geologically connected sites, where shared geological features\nsuch as pressure space and reservoir pore capacity can lead to competitive\nbehavior among stakeholders. Furthermore, CO2 storage sites are often located\nin geologically mature basins that previously served as sites for hydrocarbon\nextraction or wastewater disposal in order to leverage existing\ninfrastructures, which makes unilateral optimization even more complicated and\nunrealistic.\n  In this work, we propose a paradigm based on Markov games to quantitatively\ninvestigate how different coalition structures affect the goals of\nstakeholders. We frame this multi-stakeholder multi-site problem as a\nmulti-agent reinforcement learning problem with safety constraints. Our\napproach enables agents to learn optimal strategies while compliant with safety\nregulations. We present an example where multiple operators are injecting CO2\ninto their respective project areas in a geologically connected basin. To\naddress the high computational cost of repeated simulations of high-fidelity\nmodels, a previously developed surrogate model based on the Embed-to-Control\n(E2C) framework is employed. Our results demonstrate the effectiveness of the\nproposed framework in addressing optimal management of CO2 storage when\nmultiple stakeholders with various objectives and goals are involved.",
      "published_date": "2025-08-15",
      "content_hash": "14c7c47591e03c99b0c4eebb3d9aa316",
      "api_data": {
        "authors": [
          "Jungang Chen",
          "Seyyed A. Hosseini"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11618v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
      "link": "http://arxiv.org/abs/2508.11616v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
      "published_date": "2025-08-15",
      "content_hash": "6c53baabc337ea3c1b552c170805282a",
      "api_data": {
        "authors": [
          "Oscar Mañas",
          "Pierluca D'Oro",
          "Koustuv Sinha",
          "Adriana Romero-Soriano",
          "Michal Drozdzal",
          "Aishwarya Agrawal"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11616v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval",
      "link": "http://arxiv.org/abs/2508.11609v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.",
      "published_date": "2025-08-15",
      "content_hash": "d21691682991d0e15d276bc70ead34bf",
      "api_data": {
        "authors": [
          "Kemal Altwlkany",
          "Elmedin Selmanovic",
          "Sead Delalic"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11609v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "TinyTim: A Family of Language Models for Divergent Generation",
      "link": "http://arxiv.org/abs/2508.11607v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.",
      "published_date": "2025-08-15",
      "content_hash": "6939ab3f888a17f655cecb51104f74d7",
      "api_data": {
        "authors": [
          "Christopher J. Agostino"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11607v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Dataset Creation for Visual Entailment using Generative AI",
      "link": "http://arxiv.org/abs/2508.11605v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.",
      "published_date": "2025-08-15",
      "content_hash": "5f04e446435ce4fcec08a9723886afeb",
      "api_data": {
        "authors": [
          "Rob Reijtenbach",
          "Suzan Verberne",
          "Gijs Wijnholds"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11605v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection",
      "link": "http://arxiv.org/abs/2508.11599v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.",
      "published_date": "2025-08-15",
      "content_hash": "3006fbe3ff2806e988310d67fef07172",
      "api_data": {
        "authors": [
          "Zhihao Li",
          "Zimo Ji",
          "Tao Zheng",
          "Hao Ren",
          "Xiao Lan"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11599v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
      "link": "http://arxiv.org/abs/2508.11598v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
      "published_date": "2025-08-15",
      "content_hash": "c9a2720da3b66320f8f898bd9d8be1e7",
      "api_data": {
        "authors": [
          "Greta Tuckute",
          "Klemen Kotar",
          "Evelina Fedorenko",
          "Daniel L. K. Yamins"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11598v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Nonparametric learning of stochastic differential equations from sparse and noisy data",
      "link": "http://arxiv.org/abs/2508.11597v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "The paper proposes a systematic framework for building data-driven stochastic\ndifferential equation (SDE) models from sparse, noisy observations. Unlike\ntraditional parametric approaches, which assume a known functional form for the\ndrift, our goal here is to learn the entire drift function directly from data\nwithout strong structural assumptions, making it especially relevant in\nscientific disciplines where system dynamics are partially understood or highly\ncomplex. We cast the estimation problem as minimization of the penalized\nnegative log-likelihood functional over a reproducing kernel Hilbert space\n(RKHS). In the sparse observation regime, the presence of unobserved trajectory\nsegments makes the SDE likelihood intractable. To address this, we develop an\nExpectation-Maximization (EM) algorithm that employs a novel Sequential Monte\nCarlo (SMC) method to approximate the filtering distribution and generate Monte\nCarlo estimates of the E-step objective. The M-step then reduces to a penalized\nempirical risk minimization problem in the RKHS, whose minimizer is given by a\nfinite linear combination of kernel functions via a generalized representer\ntheorem. To control model complexity across EM iterations, we also develop a\nhybrid Bayesian variant of the algorithm that uses shrinkage priors to identify\nsignificant coefficients in the kernel expansion. We establish important\ntheoretical convergence results for both the exact and approximate EM\nsequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of\nthe drift function of stochastic dynamical systems in low-data regimes and is\nbroadly applicable across domains requiring continuous-time modeling under\nobservational constraints. We demonstrate the effectiveness of our method\nthrough a series of numerical experiments.",
      "published_date": "2025-08-15",
      "content_hash": "cd63f77a77648177d96d03d1150de23a",
      "api_data": {
        "authors": [
          "Arnab Ganguly",
          "Riten Mitra",
          "Jinpu Zhou"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11597v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "It's not a FAD: first results in using Flows for unsupervised Anomaly Detection at 40 MHz at the Large Hadron Collider",
      "link": "http://arxiv.org/abs/2508.11594v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "We present the first implementation of a Continuous Normalizing Flow (CNF)\nmodel for unsupervised anomaly detection within the realistic, high-rate\nenvironment of the Large Hadron Collider's L1 trigger systems. While CNFs\ntypically define an anomaly score via a probabilistic likelihood, calculating\nthis score requires solving an Ordinary Differential Equation, a procedure too\ncomplex for FPGA deployment. To overcome this, we propose a novel,\nhardware-friendly anomaly score defined as the squared norm of the model's\nvector field output. This score is based on the intuition that anomalous events\nrequire a larger transformation by the flow. Our model, trained via Flow\nMatching on Standard Model-like data, is synthesized for an FPGA using the\nhls4ml library. We demonstrate that our approach effectively identifies a\nvariety of beyond-the-Standard-Model signatures with performance comparable to\nexisting machine learning-based triggers. The algorithm achieves a latency of a\nfew hundred nanoseconds and requires minimal FPGA resources, establishing CNFs\nas a viable new tool for real-time, data-driven discovery at 40 MHz.",
      "published_date": "2025-08-15",
      "content_hash": "69e2468a491c6d77c74ea53c6bc84a0e",
      "api_data": {
        "authors": [
          "Francesco Vaselli",
          "Maurizio Pierini",
          "Maciej Mikolaj Glowacki",
          "Thea Aarrestad",
          "Katya Govorkova",
          "Vladimir Loncar",
          "Dimitrios Danopoulos",
          "Felice Pantaleo"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11594v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation",
      "link": "http://arxiv.org/abs/2508.11588v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.",
      "published_date": "2025-08-15",
      "content_hash": "4340c8371f29c6edc6e8fa16c59d94a3",
      "api_data": {
        "authors": [
          "Benjamin Walt",
          "Jordan Westphal",
          "Girish Krishnan"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11588v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks",
      "link": "http://arxiv.org/abs/2508.11584v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.",
      "published_date": "2025-08-15",
      "content_hash": "40983b8f94bdc46deefc616927115fe7",
      "api_data": {
        "authors": [
          "Jakub Łucki",
          "Jonathan Becktor",
          "Georgios Georgakis",
          "Robert Royce",
          "Shehryar Khattak"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11584v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models",
      "link": "http://arxiv.org/abs/2508.11582v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.",
      "published_date": "2025-08-15",
      "content_hash": "4987d738b843640e7cb777022f71145c",
      "api_data": {
        "authors": [
          "Qiguang Chen",
          "Dengyun Peng",
          "Jinhao Liu",
          "HuiKang Su",
          "Jiannan Guan",
          "Libo Qin",
          "Wanxiang Che"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11582v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Intergenerational Support for Deepfake Scams Targeting Older Adults",
      "link": "http://arxiv.org/abs/2508.11579v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "AI-enhanced scams now employ deepfake technology to produce convincing audio\nand visual impersonations of trusted family members, often grandchildren, in\nreal time. These attacks fabricate urgent scenarios, such as legal or medical\nemergencies, to socially engineer older adults into transferring money. The\nrealism of these AI-generated impersonations undermines traditional cues used\nto detect fraud, making them a powerful tool for financial exploitation. In\nthis study, we explore older adults' perceptions of these emerging threats and\ntheir responses, with a particular focus on the role of youth, who may also be\nimpacted by having their identities exploited, in supporting older family\nmembers' online safety. We conducted focus groups with 37 older adults (ages\n65+) to examine their understanding of deepfake impersonation scams and the\nvalue of intergenerational technology support. Findings suggest that older\nadults frequently rely on trusted relationships to detect scams and develop\nprotective practices. Based on this, we identify opportunities to engage youth\nas active partners in enhancing resilience across generations.",
      "published_date": "2025-08-15",
      "content_hash": "1366f74454424b1185c0e443fe11903e",
      "api_data": {
        "authors": [
          "Karina LaRubbio",
          "Alyssa Lanter",
          "Seihyun Lee",
          "Mahima Ramesh",
          "Diana Freed"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11579v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption",
      "link": "http://arxiv.org/abs/2508.11575v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources.",
      "published_date": "2025-08-15",
      "content_hash": "99ecc46f15792c4e7495e0022e85e575",
      "api_data": {
        "authors": [
          "Nges Brian Njungle",
          "Michel A. Kinsy"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11575v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles",
      "link": "http://arxiv.org/abs/2508.11574v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "The next generation networks offers significant potential to advance\nIntelligent Transportation Systems (ITS), particularly through the integration\nof Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs\nthrough efficient computing resource management remains an open challenge. This\npaper introduces a distributed computing archi tecture that integrates DTs and\nMobile Edge Computing (MEC) within a software-defined vehicular networking\nframework to enable intelligent, low-latency transportation services. A network\naware scalable collaborative task provisioning algorithm is de veloped to train\nan autonomous agent, which is evaluated using a realistic connected autonomous\nvehicle (CAV) traffic simulation. The proposed framework significantly enhances\nthe robustness and scalability of DT operations by reducing synchronization\nerrors to as low as 5% while achieving up to 99.5% utilization of edge\ncomputing resources.",
      "published_date": "2025-08-15",
      "content_hash": "4247d2db2540b3309b7de364ad3999bd",
      "api_data": {
        "authors": [
          "Mohammad Sajid Shahriar",
          "Suresh Subramaniam",
          "Motoharu Matsuura",
          "Hiroshi Hasegawa",
          "Shih-Chun Lin"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11574v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
      "link": "http://arxiv.org/abs/2508.11569v1",
      "source": "ArXiv AI Papers",
      "source_type": "arxiv",
      "category": "Research Papers",
      "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.",
      "published_date": "2025-08-15",
      "content_hash": "2c8893b00786e0c133f5858ae21bb158",
      "api_data": {
        "authors": [
          "Zheng Wang",
          "Shihao Xu",
          "Wei Shi"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11569v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition",
      "link": "http://arxiv.org/abs/2508.11624v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.",
      "published_date": "2025-08-15",
      "content_hash": "2bce1124507626a7d2db4201f51056e8",
      "api_data": {
        "authors": [
          "Niki Foteinopoulou",
          "Ignas Budvytis",
          "Stephan Liwicki"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11624v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Robust Topology and the Hausdorff-Smyth Monad on Metric Spaces over Continuous Quantales",
      "link": "http://arxiv.org/abs/2508.11623v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "We define a (preorder-enriched) category $\\mathsf{Met}$ of quantale-valued\nmetric spaces and uniformly continuous maps, with the essential requirement\nthat the quantales are continuous. For each object $(X,d,Q)$ in this category,\nwhere $X$ is the carrier set, $Q$ is a continuous quantale, and $d: X \\times X\n\\to Q$ is the metric, we consider a topology $\\tau_d$ on $X$, which generalizes\nthe open ball topology, and a topology $\\tau_{d,R}$ on the powerset\n$\\mathsf{P}(X)$, called the robust topology, which captures robustness with\nrespect to small perturbations of parameters. We define a (preorder-enriched)\nmonad $\\mathsf{P}_S$ on $\\mathsf{Met}$, called the Hausdorff-Smyth monad, which\ncaptures the robust topology, in the sense that the open ball topology of the\nobject $\\mathsf{P}_S(X,d,Q)$ coincides with the robust topology $\\tau_{d,R}$\nfor the object $(X,d,Q)$. We prove that every topology arises from a\nquantale-valued metric. As such, our framework provides a foundation for\nquantitative reasoning about imprecision and robustness in a wide range of\ncomputational and physical systems.",
      "published_date": "2025-08-15",
      "content_hash": "a25383f215be1711d27b1eb85aad7793",
      "api_data": {
        "authors": [
          "Francesco Dagnino",
          "Amin Farjudian Eugenio Moggi"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11623v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Higher Zariski Geometry",
      "link": "http://arxiv.org/abs/2508.11621v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "We revisit the classical constructions of tensor-triangular geometry in the\nsetting of stably symmetric monoidal idempotent-complete $\\infty$-categories,\nhenceforth referred to as 2-rings. In this setting, we produce a Zariski\ntopology, a Zariski spectrum, a category of locally 2-ringed spaces (more\ngenerally $\\infty$-topoi), and an affine spectrum-global sections adjunction,\nbased on the framework of ``$\\infty$-topoi with geometric structure'' as\ndeveloped by Lurie in \\cite{LurieDAG5}. Using work of Kock and Pitsch, we\ncompute that the underlying space of the Zariski spectrum of a 2-ring recovers\nthe Balmer spectrum of its homotopy category. These constructions mirror the\nanalogous structures in the classical Zariski geometry of commutative rings\n(and commutative ring spectra), and we also demonstrate additional\ncompatibility between classical Zariski and higher Zariski geometry. For rigid\n2-rings, we show that the descent results of Balmer and Favi admit coherent\nenhancements. As a corollary, we obtain that the Zariski spectrum fully\nfaithfully embeds rigid 2-rings into locally 2-ringed $\\infty$-topoi. In an\nappendix, we prove a ``stalk-locality principle'' for the telescope conjecture\nin the rigid setting, extending earlier work of Hrbek.",
      "published_date": "2025-08-15",
      "content_hash": "29da287fbfff3c063e20c26f71d108d2",
      "api_data": {
        "authors": [
          "Ko Aoki",
          "Tobias Barthel",
          "Anish Chedalavada",
          "Tomer Schlank",
          "Greg Stevenson"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11621v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Two-Impulse Trajectory Design in Two-Body Systems With Riemannian Geometry",
      "link": "http://arxiv.org/abs/2508.11612v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "This work presents a new method for generating impulsive trajectories in\nrestricted two-body systems by leveraging Riemannian geometry. The proposed\nmethod transforms the standard trajectory optimization problem into a purely\ngeometric one that involves computing a set of geodesics for a suitable\nRiemannian metric. This transformation is achieved by defining a metric,\nspecifically the Jacobi metric, that embeds the dynamics directly into the\nmetric, so any geodesic of the metric is also a dynamically feasible\ntrajectory. The method finds the fuel-optimal transfer trajectory by sampling\ncandidate energy ($\\Delta V$) changes for different points on the current and\ndesired orbit, and efficiently computing and evaluating each candidate\ngeodesic, which are equivalent to candidate orbit transfer trajectories via the\nJacobi metric. The method bypasses the known issues of optimization-based\nmethods, e.g., sensitivity to the initial guess, and can be applied to more\ncomplex two-body systems. The approach is demonstrated on the minimum-$\\Delta\nV$ two-impulse phase-free orbit transfer problem, first on a Keplerian system\nand second on a system with a modeled $J_2$ perturbation. The proposed method\nis shown to meet or exceed the state-of-the-art methods in the minimum-$\\Delta\nV$ problem in the Keplerian system. The generality and versatility of the\napproach is demonstrated by seamlessly including the $J_2$ perturbation, a case\nthat many existing methods cannot handle. Numerical simulations and performance\ncomparisons showcase the effectiveness of the approach.",
      "published_date": "2025-08-15",
      "content_hash": "a4041f38cecca3bf56890796b6a80b74",
      "api_data": {
        "authors": [
          "Samuel G. Gessow",
          "James Tseng",
          "Eden Zafran",
          "Brett T. Lopez"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11612v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Exponentiable virtual double categories and presheaves for double categories",
      "link": "http://arxiv.org/abs/2508.11611v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "Given a pair of pseudo double categories $\\mathbb A$ and $\\mathbb B$, the lax\nfunctors from $\\mathbb A$ to $\\mathbb B$, along with their transformations,\nmodules, and multimodulations, assemble into a virtual double category\n$\\mathbf{\\mathbb Lax}(\\mathbb A, \\mathbb B)$. We exhibit a universal property\nof this construction by observing that it arises naturally from the\nconsideration of exponentiability for virtual double categories. In particular,\nwe show that every pseudo double category is exponentiable as a virtual double\ncategory, whereby the virtual double category $\\mathbf{\\mathbb Lax}(\\mathbb A,\n\\mathbb B)$ of lax functors arises as the virtual double category\n$\\mathbf{\\mathbb Mod}(\\mathbb B^{\\mathbb A})$ of monads and modules in the\nexponential $\\mathbb B^{\\mathbb A}$. We explore some consequences of this\ncharacterisation, demonstrating that it facilitates simple proofs of statements\nthat heretofore required unwieldy computations. For instance, we deduce that\nthe 2-category of pseudo double categories and lax functors is enriched in the\n2-category of normal virtual double categories, and demonstrate that several\naspects of the Yoneda theory of pseudo double categories - such as the\ncorrespondence between presheaves and discrete fibrations - are substantially\nsimplified by this perspective.",
      "published_date": "2025-08-15",
      "content_hash": "8e6b9ea3c647170a86e53b137b57ada8",
      "api_data": {
        "authors": [
          "Nathanael Arkor"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11611v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Quantum Simulation of Collective Neutrino Oscillations in Dense Neutrino Environment",
      "link": "http://arxiv.org/abs/2508.11610v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "Inside dense neutrino gases, such as neutron star mergers or core-collapse\nsupernovae, collective neutrino effects cause the transformation of one\nneutrino flavour into another. Due to strong neutrino self-interactions in\nthese environments, there is prevalence of flavour swapping. Considering these\nenvironments to be isotropic and homogeneous, we present a study of collective\nneutrino oscillations by simulating such a system on a noisy quantum simulator\n(Qiskit AerSimulator) and a quantum processor (ibm\\_brisbane). We model the\neffective Hamiltonian governing neutrino interactions and by applying the\nTrotter-Suzuki approximation, decompose it into a tractable form suitable for\nquantum circuit implementation of the time-evolution propagator. Encoding the\nneutrino state for a system of two- and three-neutrinos onto qubits, we compute\nthe time evolution of the inversion probability relative to the initial product\nstate. Furthermore, we present quantum circuits to evaluate the concurrence as\na measure of entanglement between the neutrinos.",
      "published_date": "2025-08-15",
      "content_hash": "9881943d52594b2222f2eec7c598954f",
      "api_data": {
        "authors": [
          "Shvetaank Tripathi",
          "Sandeep Joshi",
          "Garima Rajpoot",
          "Prashant Shukla"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11610v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "A multigrid method for CutFEM and its implementation on GPU",
      "link": "http://arxiv.org/abs/2508.11608v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "We present a multigrid method for an unfitted finite element discretization\nof the Dirichlet boundary value problem. The discretization employs Nitsche's\nmethod to implement the boundary condition and additional face based ghost\npenalties for stabilization. We apply standard intergrid operators, relying on\nthe fact that the relevant domain of computation does not grow under mesh\nrefinement. The smoother is a parallel implementation of the multiplicative\nvertex-patch smoother with inconsistent treatment of ghost penalties. Our\ncomputational results show that we obtain a fast converging method.\nFurthermore, runtime comparison to fitted methods show that the losses are\nmoderate although many optimizations for Cartesian vertex patches cannot be\napplied on cut patches.",
      "published_date": "2025-08-15",
      "content_hash": "1ddaccd56d213308d2833eb1cfda6c6d",
      "api_data": {
        "authors": [
          "Cu Cui",
          "Guido Kanschat"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11608v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion",
      "link": "http://arxiv.org/abs/2508.11603v1",
      "source": "ArXiv Computer Vision",
      "source_type": "arxiv",
      "category": "Computer Vision",
      "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.",
      "published_date": "2025-08-15",
      "content_hash": "79f98819cecfe66bc6f72126f2267ba0",
      "api_data": {
        "authors": [
          "Zhe Zhu",
          "Honghua Chen",
          "Peng Li",
          "Mingqiang Wei"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11603v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Laser Interferometer Lunar Antenna (LILA): Advancing the U.S. Priorities in Gravitational-wave and Lunar Science",
      "link": "http://arxiv.org/abs/2508.11631v1",
      "source": "ArXiv NLP Papers",
      "source_type": "arxiv",
      "category": "Natural Language Processing",
      "summary": "The Laser Interferometer Lunar Antenna (LILA) is a next-generation\ngravitational-wave (GW) facility on the Moon. By harnessing the Moon's unique\nenvironment, LILA fills a critical observational gap in the mid-band GW\nspectrum ($0.1 - 10$ Hz) between terrestrial detectors (LIGO, Virgo, KAGRA) and\nthe future space mission LISA. Observations enabled by LILA will fundamentally\ntransform multi-messenger astrophysics and GW probes of fundamental physics.\nLILA will measure the lunar deep interior better than any existing planetary\nseismic instruments. The LILA mission is designed for phased development\naligned with capabilities of the U.S.'s Commercial Lunar Payload Services and\nArtemis programs. LILA is a unique collaboration between universities, space\nindustries, U.S. government laboratories, and international partners.",
      "published_date": "2025-08-15",
      "content_hash": "3eab165f2179b499769d73555ccdaef0",
      "api_data": {
        "authors": [
          "Karan Jani",
          "Matthew Abernathy",
          "Emanuele Berti",
          "Valerio Boschi",
          "Sukanya Chakrabarti",
          "Alice Cocoros",
          "John W. Conklin",
          "Teviet Creighton",
          "Simone Dell'Agnello",
          "Jean-Claude Diels",
          "Stephen Eikenberry",
          "T. Marshall Eubanks",
          "Kiranjyot Gill",
          "Jonathan E. Grindlay",
          "Kris Izquierdo",
          "Jaesung Lee",
          "Abraham Loeb",
          "Philippe Lognonné",
          "Francesco Longo",
          "Manuel Pichardo Marcano",
          "Mark Panning",
          "Paula do Vale Pereira",
          "Volker Quetschke",
          "Ashique Rahman",
          "Massimiliano Razzano",
          "Robert Reed",
          "Brett Shapiro",
          "David Shoemaker",
          "William Smith",
          "James Trippe",
          "Eric Van Stryland",
          "Wan Wu",
          "Anjali B. Yelikar"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11631v1",
        "journal_ref": null,
        "doi": null
      }
    },
    {
      "title": "Approximate Factor Model with S-vine Copula Structure",
      "link": "http://arxiv.org/abs/2508.11619v1",
      "source": "ArXiv NLP Papers",
      "source_type": "arxiv",
      "category": "Natural Language Processing",
      "summary": "We propose a novel framework for approximate factor models that integrates an\nS-vine copula structure to capture complex dependencies among common factors.\nOur estimation procedure proceeds in two steps: first, we apply principal\ncomponent analysis (PCA) to extract the factors; second, we employ maximum\nlikelihood estimation that combines kernel density estimation for the margins\nwith an S-vine copula to model the dependence structure. Jointly fitting the\nS-vine copula with the margins yields an oblique factor rotation without\nresorting to ad hoc restrictions or traditional projection pursuit methods. Our\ntheoretical contributions include establishing the consistency of the rotation\nand copula parameter estimators, developing asymptotic theory for the\nfactor-projected empirical process under dependent data, and proving the\nuniform consistency of the projected entropy estimators. Simulation studies\ndemonstrate convergence with respect to both the dimensionality and the sample\nsize. We further assess model performance through Value-at-Risk (VaR)\nestimation via Monte Carlo methods and apply our methodology to the daily\nreturns of S&P 500 Index constituents to forecast the VaR of S&P 500 index.",
      "published_date": "2025-08-15",
      "content_hash": "5c92b7bf077bd670a4760ba6c895227f",
      "api_data": {
        "authors": [
          "Jialing Han",
          "Yu-Ning Li"
        ],
        "pdf_url": "http://arxiv.org/pdf/2508.11619v1",
        "journal_ref": null,
        "doi": null
      }
    }
  ]
}