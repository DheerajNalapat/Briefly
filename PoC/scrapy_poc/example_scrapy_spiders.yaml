# Example Scrapy Spiders Configuration
# This file shows how to configure different Scrapy spiders for the crawler

scrapy_spiders:
  # Technology News Spiders
  - name: "techcrunch_ai"
    description: "TechCrunch AI news spider"
    domain: "techcrunch.com"
    start_url: "https://techcrunch.com/tag/artificial-intelligence/"
    category: "Technology"
    max_items: 10
    download_delay: 2
    enabled: true

  - name: "venturebeat_ai"
    description: "VentureBeat AI news spider"
    domain: "venturebeat.com"
    start_url: "https://venturebeat.com/category/ai/"
    category: "Technology"
    max_items: 10
    download_delay: 2
    enabled: true

  - name: "mit_tech_review"
    description: "MIT Technology Review AI spider"
    domain: "technologyreview.com"
    start_url: "https://www.technologyreview.com/topic/artificial-intelligence/"
    category: "Technology"
    max_items: 8
    download_delay: 3
    enabled: true

  # Research and Academic Spiders
  - name: "arxiv_ai"
    description: "arXiv AI papers spider"
    domain: "arxiv.org"
    start_url: "https://arxiv.org/list/cs.AI/recent"
    category: "Research"
    max_items: 15
    download_delay: 5
    enabled: true

  - name: "nature_ai"
    description: "Nature AI research spider"
    domain: "nature.com"
    start_url: "https://www.nature.com/subjects/artificial-intelligence"
    category: "Academic"
    max_items: 8
    download_delay: 4
    enabled: true

  # Blog and Opinion Spiders
  - name: "medium_ai"
    description: "Medium AI articles spider"
    domain: "medium.com"
    start_url: "https://medium.com/tag/artificial-intelligence"
    category: "Blog"
    max_items: 8
    download_delay: 2
    enabled: true

  - name: "towards_data_science"
    description: "Towards Data Science AI spider"
    domain: "towardsdatascience.com"
    start_url: "https://towardsdatascience.com/tagged/artificial-intelligence"
    category: "Blog"
    max_items: 10
    download_delay: 2
    enabled: true
# Configuration Options:
# - name: Spider identifier (must match spider class names)
# - description: Human-readable description of the spider
# - domain: Domain to crawl (for robots.txt compliance)
# - start_url: Starting URL for the spider
# - category: Category for grouping news items
# - max_items: Maximum number of items to extract
# - download_delay: Delay between requests (seconds)
# - enabled: Whether this spider is active

# Download Delays (in seconds):
# - 1-2 = Fast crawling (be respectful!)
# - 3-4 = Moderate speed (recommended)
# - 5+ = Slow, very respectful (for research sites)

# Note: The actual spider classes must be implemented in the code
# to match these configuration names.
